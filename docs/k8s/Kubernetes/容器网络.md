# 容器网络

## 网络栈

- 网卡（Network Interface）
- 回环设备（Loopback Device）
- 路由表（Routing Table）
- iptables 规则

## 容器网络的实现原理

- 网桥（Bridge）

  - 在 Linux 中，能够起到虚拟交换机作用的网络设备
  - 工作在数据链路层（Data Link）的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端口（Port）上
  - Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信

- Veth Pair

  - 它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里
  - 这就使得 Veth Pair 常常被用作连接不同 Network Namespace 的“网线”

- 单节点容器间通信

  - 容器 1 通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查找对应的 MAC 地址
  - docker0 网桥就会扮演二层交换机的角色，把 ARP 广播转发到其他被“插”在 docker0 上的虚拟网卡上
  - 同样连接在 docker0 上的容器 2 的网络协议栈就会收到这个 ARP 请求，从而将 IP 所对应的 MAC 地址回复给容器 1
  - 有了这个目的 MAC 地址，容器 1 的 eth0 网卡就可以将数据包发出去；数据包会经过 docker0 转发到容器 2

- 当一个容器试图连接到另外一个宿主机时

  - 容器 1 发出的请求数据包，首先经过 docker0 网桥出现在宿主机上
  - 然后根据宿主机的路由表里的直连路由规则，对另一个宿主机的访问请求就会交给宿主机的 eth0 处理
  - 这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达指定的宿主机上

- 跨主通信
  - Overlay Network（覆盖网络）
    - 我们需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络

## Flannel - CoreOS 公司主推的容器网络方案

### UDP 模式

- IP 包从容器经过 docker0 出现在宿主机

  - flannel0：它是一个 TUN 设备（Tunnel 设备）；
  - TUN 设备：在操作系统内核和用户应用程序之间传递 IP 包

- 然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP 包

- 然后，flanneld 看到了这个 IP 包的目的地址，就把它发送给了 Node 2 宿主机

  - 在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”
  - 子网与宿主机的对应关系，正是保存在 Etcd 当中
  - flanneld 进程可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24）
  - flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2
  - Node 2 的 flanneld 收到数据包之后，会将包传给 docker0，再到容器 2

- 缺点
  - Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。在 Linux 操作系统中，上述这些上下文切换和用户态操作的代价其实是比较高的

### VXLAN 模式

- 即 Virtual Extensible LAN（虚拟可扩展局域网）；是 Linux 内核本身就支持的一种网络虚似化技术。VXLAN 可以完全在内核态实现上述封装和解封装的工作

- VTEP 设备：VXLAN Tunnel End Point（虚拟隧道端点）
  - 它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的
  - VTEP 设备之间，就需要想办法组成一个虚拟的二层网络，即：通过二层数据帧进行通信
  - “源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”
    - Flannel 会在每台节点启动时把它的 VTEP 设备对应的 ARP 记录，直接下放到其他每台宿主机上
  - 然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去
  - 这个 UDP 包该发给哪台宿主机呢
    - flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发
    - 在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库
    - 这个 flannel.1“网桥”对应的 FDB 信息，也是 flanneld 进程负责维护的，可以使用“目的 VTEP 设备”的 MAC 地址进行查询目标设备 IP

## Kubernetes 网络模型与 CNI 网络插件

- Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0
- CNI 网桥只是接管所有 CNI 插件负责的、即 Kubernetes 创建的容器（Pod）
- 使用 CNI 的原因
  - Kubernetes 项目并没有使用 Docker 的网络模型（CNM），所以它并不希望、也不具备配置 docker0 网桥的能力
  - 另一方面，这还与 Kubernetes 如何配置 Pod，也就是 Infra 容器的 Network Namespace 密切相关
    - Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 Network Namespace，配置符合预期的网络栈
- CNI 的基础可执行文件
  - 第一类，Main 插件，它是用来创建具体网络设备的二进制文件
    - 比如，bridge（网桥设备）、ipvlan、loopback（lo 设备）、macvlan、ptp（Veth Pair 设备），以及 vlan
  - 第二类，IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件
    - 比如，dhcp，这个文件会向 DHCP 服务器发起请求；host-local，则会使用预先配置的 IP 地址段来进行分配
  - 第三类，是由 CNI 社区维护的内置 CNI 插件
    - 比如：flannel，就是专门为 Flannel 项目提供的 CNI 插件；tuning，是一个通过 sysctl 调整网络设备参数的二进制文件；portmap，是一个通过 iptables 配置端口映射的二进制文件；bandwidth，是一个使用 Token Bucket Filter (TBF) 来进行限流的二进制文件
