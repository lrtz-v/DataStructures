# 网络性能

## Linux 网络栈

- 最上层的应用程序，需要通过系统调用，来跟套接字接口进行交互
- 套接字的下面，就是我们前面提到的传输层、网络层和网络接口层
- 最底层，则是网卡驱动程序以及物理网卡设备

## Linux 网络收发流程

### 网络包的接收流程

- 当一个网络帧到达网卡后，网卡把这个网络包放到收包队列中；然后通过硬中断，告诉中断处理程序已经收到了网络包
- 网卡中断处理程序会为网络帧分配内核数据结构（sk_buff），并将其拷贝到 sk_buff 缓冲区中；然后再通过软中断，通知内核收到了新的网络帧
- 内核协议栈从缓冲区中取出网络帧，并通过网络协议栈，从下到上逐层处理这个网络帧
  - 在链路层检查报文的合法性，找出上层协议的类型（比如 IPv4 还是 IPv6），再去掉帧头、帧尾，然后交给网络层
  - 网络层取出 IP 头，判断网络包下一步的走向，比如是交给上层处理还是转发。当网络层确认这个包是要发送到本机后，就会取出上层协议的类型（比如 TCP 还是 UDP），去掉 IP 头，再交给传输层处理
  - 传输层取出 TCP 头或者 UDP 头后，根据 < 源 IP、源端口、目的 IP、目的端口 > 四元组作为标识，找出对应的 Socket，并把数据拷贝到 Socket 的接收缓存中
  - 应用程序就可以使用 Socket 接口，读取到新接收到的数据了

### 网络包的发送流程

- 应用程序调用 Socket API（比如 sendmsg）发送网络包
- 由于这是一个系统调用，所以会陷入到内核态的套接字层中。套接字层会把数据包放到 Socket 发送缓冲区中
- 网络协议栈从 Socket 发送缓冲区中，取出数据包；再按照 TCP/IP 栈，从上到下逐层处理
  - 传输层和网络层，分别为其增加 TCP 头和 IP 头，执行路由查找确认下一跳的 IP，并按照 MTU 大小进行分片
  - 分片后的网络包，再送到网络接口层，进行物理地址寻址，以找到下一跳的 MAC 地址。然后添加帧头和帧尾，放到发包队列中
  - 通过软中断通知驱动程序：发包队列中有新的网络帧需要发送
  - 驱动程序从发包队列中读出网络帧，并通过物理网卡把它发送出去

## 性能指标

- 带宽，表示链路的最大传输速率，单位通常为 b/s （比特 / 秒）
- 吞吐量，表示单位时间内成功传输的数据量，单位通常为 b/s（比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽限制，而吞吐量 / 带宽，也就是该网络的使用率
- 延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT）
- PPS，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，比如硬件交换机，通常可以达到线性转发（即 PPS 可以达到或者接近理论最大值）。而基于 Linux 服务器的转发，则容易受网络包大小的影响。

### 网络配置

- ifconfig 和 ip 只显示了网络接口收发数据包的统计信息

### 套接字信息

- netstat 或者 ss ，查看套接字、网络栈、网络接口以及路由表的信息

### 协议栈统计信息

- netstat ，查看协议栈的信息来
- ss
  - 当套接字处于连接状态（Established）时
    - Recv-Q 表示套接字缓冲还没有被应用程序取走的字节数（即接收队列长度）
    - Send-Q 表示还没有被远端主机确认的字节数（即发送队列长度）
  - 当套接字处于监听状态（Listening）时
    - Recv-Q 表示全连接队列的长度
    - Send-Q 表示全连接队列的最大长度

### 网络吞吐和 PPS

- sar 增加 -n 参数就可以查看网络的统计信息

### 连通性和延时

- 使用 ping ，来测试远程主机的连通性和延时，而这基于 ICMP 协议

## I/O 模型优化

### I/O 事件通知

- 水平触发：只要文件描述符可以非阻塞地执行 I/O ，就会触发通知。也就是说，应用程序可以随时检查文件描述符的状态，然后再根据状态，进行 I/O 操作
- 边缘触发：只有在文件描述符的状态发生改变（也就是 I/O 请求达到）时，才发送一次通知。这时候，应用程序需要尽可能多地执行 I/O，直到无法继续读写，才可以停止。如果 I/O 没执行完，或者因为某种原因没来得及处理，那么这次通知也就丢失了

### I/O 多路复用

- 使用非阻塞 I/O 和水平触发通知，比如使用 select 或者 poll
  - select 和 poll 需要从文件描述符列表中，找出哪些可以执行 I/O ，然后进行真正的网络 I/O 读写
  - 由于 I/O 是非阻塞的，一个线程中就可以同时监控一批套接字的文件描述符，这样就达到了单线程处理多请求的目的
  - 需要对这些文件描述符列表进行轮询，请求数多的时候就会比较耗时
  - 每次调用 select 和 poll 时，还需要把文件描述符的集合，从用户空间传入内核空间，由内核修改后，再传出到用户空间中
- 使用非阻塞 I/O 和边缘触发通知，比如 epoll
  - epoll 使用红黑树，在内核中管理文件描述符的集合，这样，就不需要应用程序在每次操作时都传入、传出这个集合
  - epoll 使用事件驱动的机制，只关注有 I/O 事件发生的文件描述符，不需要轮询扫描整个集合
- 使用异步 I/O（Asynchronous I/O，简称为 AIO）
  - 异步 I/O 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成。而在 I/O 完成后，系统会用事件通知（比如信号或者回调函数）的方式，告诉应用程序。这时，应用程序才会去查询 I/O 操作的结果

### 工作模型优化

- 第一种，主进程 + 多个 worker 子进程，这也是最常用的一种模型
  - 主进程执行 bind() + listen() 后，创建并管理多个子进程，子进程没任务时休眠，有任务时唤醒
  - 在每个子进程中，都通过 accept() 或 epoll_wait() ，来处理相同的套接字
- 第二种，监听到相同端口的多进程模型
  - 所有的进程都监听相同的端口，并且开启 SO_REUSEPORT 选项，由内核负责将请求负载均衡到这些监听进程中去。这一过程如下图所示
