# IO

## 文件系统与磁盘的区别

- 磁盘是存储数据的块设备，也是文件系统的载体。所以，文件系统确实还是要通过磁盘，来保证数据的持久化存储
- 在读写普通文件时，I/O 请求会首先经过文件系统，然后由文件系统负责，来与磁盘进行交互
  - Cache：文件系统管理的缓存
- 在读写块设备文件时，会跳过文件系统，直接与磁盘交互，也就是所谓的“裸 I/O”
  - Buffer： 裸磁盘的缓存

## 文件系统

- 文件系统，本身是对存储设备上的文件，进行组织管理的机制；组织方式不同，就会形成不同的文件系统

### 索引节点和目录项

- 索引节点
  - 简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等
  - 索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中；索引节点同样占用磁盘空间
- 目录项
  - 简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系
  - 多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存

### 虚拟文件系统

- Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 VFS（Virtual File System）
  - VFS 定义了一组所有文件系统都支持的数据结构和标准接口

### 文件系统 I/O

- 第一种，根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O
  - 缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件
  - 非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存
- 第二，根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O
  - 直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件
  - 非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘
- 第三，根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O
  - 阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务
  - 非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果
- 第四，根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O
  - 同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得 I/O 响应
  - 异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序

### 性能观测

- 容量
  - 通过 df 查询文件系统的磁盘空间使用情况
- 缓存
  - 用 free 或 vmstat，来观察页缓存的大小
  - 使用 slabtop ，来找到占用内存最多的缓存类型

## 磁盘 I/O

### 磁盘

- 磁盘实际上是作为一个块设备来管理的，也就是以块为单位读写数据，并且支持随机读写

### 通用块层

- 处在文件系统和磁盘驱动中间的一个块设备抽象层
  - 向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序
  - 会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率
    - 对 I/O 请求排序的过程，也就是我们熟悉的 I/O 调度
      - Linux 内核支持四种 I/O 调度算法，分别是 NONE、NOOP、CFQ 以及 DeadLine
        - NONE ，更确切来说，并不能算 I/O 调度算法。因为它完全不使用任何 I/O 调度器，对文件系统和应用程序的 I/O 其实不做任何处理，常用在虚拟机中（此时磁盘 I/O 调度完全由物理机负责）
        - NOOP ，是最简单的一种 I/O 调度算法。它实际上是一个先入先出的队列，只做一些最基本的请求合并，常用于 SSD 磁盘
        - CFQ（Completely Fair Scheduler），也被称为完全公平调度器，是现在很多发行版的默认 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求
        - DeadLine 调度算法，分别为读、写请求创建了不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限（deadline）的请求被优先处理

### I/O 栈

- [Linux Storage Stack Diagram](./Linux Storage Stack Diagram.png)
- 文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据
- 通用块层，包括块设备 I/O 队列和 I/O 调度器。它会对文件系统的 I/O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层
- 设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作

## 磁盘性能指标

- 基本指标
  - 使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈
  - 饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求
  - IOPS（Input/Output Per Second），是指每秒的 I/O 请求数
  - 吞吐量，是指每秒的 I/O 请求大小
  - 响应时间，是指 I/O 请求从发出到收到响应的间隔时间
- 磁盘 I/O 观测
  - iostat 提供了每个磁盘的使用率、IOPS、吞吐量等各种常见的性能指标，当然，这些指标实际上来自 /proc/diskstats
  - pidstat/iotop 进程 I/O 观测

## I/O 模型

### 同步阻塞 I/O（blocking I/O）

- 在等待数据到数据复制的两个阶段，整个进程都被阻塞
- blocking IO 的特点就是在 IO 执行的两个阶段都被 block 了

### 同步非阻塞 I/O（nonblocking I/O）

- 内核马上返回给进程，如果数据还没准备好，此时会返回一个 error。进程在返回之后，可以干点别的事情，然后再发起 recvform 系统调用。重复上面的过程（polling）
- 直到数据准备好，再拷贝数据到进程，进行数据处理。但拷贝数据整个过程，进程仍然是属于阻塞的状态
- 优点：能够在等待任务完成的时间里干其他活了
- 缺点：任务完成的响应延迟增大了，轮询会消耗大量的 CPU 时间

### I/O 多路复用（ I/O multiplexing）

- 通过一种机制一个进程能同时等待多个 I/O 文件描述符
- 监视的方式分为 select，poll，epoll
- select/epoll 的好处就在于单个 process 就可以同时处理多个网络连接的 IO
- 当用户进程调用了 select，那么整个进程会被 block，当任何一个 socket 中的数据准备好了，select 就会返回
- select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接
- I/O 多路复用是阻塞在 select，epoll 这样的系统调用之上，而没有阻塞在真正的 I/O 系统调用如 recvfrom 之上
- I/O 多路复用技术通过把多个 I/O 的阻塞复用到同一个 select 的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求
- I/O 多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程
- 同步阻塞

### 信号驱动式 I/O（signal-driven I/O）

- 允许 Socket 进行信号驱动 IO,并安装一个信号处理函数，进程继续运行并不阻塞
- 当数据准备好时，进程会收到一个 SIGIO 信号，可以在信号处理函数中调用 I/O 操作函数处理数据

### 异步非阻塞 I/O（asynchronous I/O）

- 异步 IO 不是顺序执行
- 用户进程进行 aio_read 系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情
- kernel 会等待数据准备完成，然后将数据拷贝到用户内存
- 然后 kernel 会给用户进程发送一个 signal 或执行一个基于线程的回调函数来完成这次 I/O 处理过程
- IO 两个阶段，进程都是非阻塞的
- 异步 IO 库，例如 libevent、libev、libuv

### 异步阻塞

- 使用阻塞的理由一：某些场景下，需要顺序执行
- 使用阻塞的理由二：降低响应延迟。异步任务处理完成时，进程可能在处理其他任务中，任务切换需要时间
- 为了在异步环境里模拟 “顺序执行” 的效果，就需要把同步代码转换成异步形式，这称为 CPS（Continuation Passing Style）变换
- 用户只需用同步方式编写代码，CPS 变换器会把它转换成层层嵌套的异步回调形式

## 总结

- blocking I/O，non-blocking I/O，I/O multiplexing 都属于 synchronous I/O
- SIGIO，AIO 都属于 asynchronous I/O
- select/poll/epoll 本身是同步的，可以阻塞也可以不阻塞

### select

- int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout)
- 可以设置 timeval 决定该系统调用是否阻塞，设置为 NULL 即阻塞，为 0 不阻塞
- 单个进程所打开的 FD 是有限制的，它由 FD_SETSIZE 设置，32 位机默认是 1024 个。64 位机默认是 2048。一般来说这个数目和系统内存关系很大，具体数目可以 cat /proc/sys/fs/file-max 察看
- 对 socket 进行扫描时是线性扫描，即采用轮询的方法，效率较低。
- 需要维护一个用来存放大量 fd 的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大

### poll

- int poll(struct pollfd \*fds, nfds_t nfds, int timeout)
- 可以通过指定 timeout 的值来决定是否阻塞（当 timeout ＜ 0 时，会无限期阻塞；当 timeout=0 时，会立即返回）
- 它没有最大连接数的限制，原因是它是基于链表来存储的
- 大量的 fd 的数组被整体复制于用户态和内核地址空间之间
- poll 还有一个特点是“水平触发”，如果报告了 fd 后，没有被处理，那么下次 poll 时会再次报告该 fd

### epoll

- epoll_wait(int epfd, struct epoll_event \*events, int maxevents, int timeout)
- 可以通过指定 timeout 来指定该调用是否阻塞（当 timeout=-1 时，会无限期阻塞；当 timeout=0 时，会立即返回；>-时，阻塞 T 毫秒）
- epoll 使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的 copy 只需一次
- 没有最大并发连接的限制，能打开的 FD 的上限远大于 1024（1G 的内存上能监听约 10 万个端口）
- 效率提升，不是轮询的方式，不会随着 FD 数目的增加效率下降
- 内存拷贝，利用 mmap()文件映射内存加速与内核空间的消息传递；即 epoll 使用 mmap 减少复制开销
- epoll 对文件描述符的操作有两种模式：水平触发（level trigger）和 边缘触发（edge trigger）

#### 水平触发

- 当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait 时，会再次响应应用程序并通知此事件。
- 同时支持 block 和 no-block socket

#### 边缘触发

- 当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait 时，不会再次响应应用程序并通知此事件。
- 只支持 no-block socket

### Finally

- 表面上看 epoll 的性能最好，但是在连接数少并且连接都十分活跃的情况下，select 和 poll 的性能可能比 epoll 好，毕竟 epoll 的通知机制需要很多函数回调。
- select 低效是因为每次它都需要轮询。但低效也是相对的，视情况而定，也可通过良好的设计改善。

## 工具

- df 查看磁盘空间
- slabtop
- iostat 看磁盘的 await，utils，iops，bandwidth
